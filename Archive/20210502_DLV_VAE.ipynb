{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20210502_DLV_VAE.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPrDnqhT895I2RUDzWzgpO3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"93bf01f74a124edd81488bff6561ad46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_721f617f1a374fa88b9c0aef38bc9e7b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c2021b15492741a39f47cd0f8a21e870","IPY_MODEL_ded99d46f6c54494bfdfa0f4345f6af1"]}},"721f617f1a374fa88b9c0aef38bc9e7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c2021b15492741a39f47cd0f8a21e870":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4299935731194575909af7b5b40054a7","_dom_classes":[],"description":"Dl Completed...:  25%","_model_name":"FloatProgressModel","bar_style":"danger","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dacd61287f2a406aa4ab20c3342ac04c"}},"ded99d46f6c54494bfdfa0f4345f6af1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_131998e905ee4e5393b459361aefd80f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/4 [00:01&lt;00:03,  1.20s/ url]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2a38074dadcb4f0a8615d4071750f6bd"}},"4299935731194575909af7b5b40054a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dacd61287f2a406aa4ab20c3342ac04c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"131998e905ee4e5393b459361aefd80f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2a38074dadcb4f0a8615d4071750f6bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"059815df872d490dad8bc61e6131a09c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_75bf74fc392741b5ba9661e37e5c1f9c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2786342aa4764907b491603aaa4cecfc","IPY_MODEL_c4f8a46e9d93456fba542e5306002500"]}},"75bf74fc392741b5ba9661e37e5c1f9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2786342aa4764907b491603aaa4cecfc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fd6a8ee8ea65434c9901ddd747882e67","_dom_classes":[],"description":"Dl Size...: ","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_74b67cc268b049aab3f8b40c45e5d418"}},"c4f8a46e9d93456fba542e5306002500":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_75966777331c447099347686b70b85da","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/0 [00:01&lt;?, ? MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_35d9de2455f34792b8cafdbadab18368"}},"fd6a8ee8ea65434c9901ddd747882e67":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"74b67cc268b049aab3f8b40c45e5d418":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"75966777331c447099347686b70b85da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"35d9de2455f34792b8cafdbadab18368":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"ghnbN4AIe3Vs"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_datasets as tfds\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython import display\n","import os\n","import shutil\n","from shutil import copyfile\n","import random\n","import zipfile"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldm019dsuAF7"},"source":["# Load the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":663,"referenced_widgets":["93bf01f74a124edd81488bff6561ad46","721f617f1a374fa88b9c0aef38bc9e7b","c2021b15492741a39f47cd0f8a21e870","ded99d46f6c54494bfdfa0f4345f6af1","4299935731194575909af7b5b40054a7","dacd61287f2a406aa4ab20c3342ac04c","131998e905ee4e5393b459361aefd80f","2a38074dadcb4f0a8615d4071750f6bd","059815df872d490dad8bc61e6131a09c","75bf74fc392741b5ba9661e37e5c1f9c","2786342aa4764907b491603aaa4cecfc","c4f8a46e9d93456fba542e5306002500","fd6a8ee8ea65434c9901ddd747882e67","74b67cc268b049aab3f8b40c45e5d418","75966777331c447099347686b70b85da","35d9de2455f34792b8cafdbadab18368"]},"id":"vJlU6CXIrApk","executionInfo":{"status":"error","timestamp":1619952227230,"user_tz":-120,"elapsed":2407,"user":{"displayName":"Xiying Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5qAohoDgkfB_l_JjwVU3bU1D3p1gYIAZX2aBbXA=s64","userId":"17944844107397903410"}},"outputId":"3167c44b-59da-45f2-ed23-02e26c26a998"},"source":["# First try: load the dataset from TensorFlow dataset => failed\n","(ds_train_raw, ds_test_raw), ds_info = tfds.load(\n","    'celeb_a',\n","    split=['train', 'test'],\n","    shuffle_files=True,\n","    with_info=True\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1mDownloading and preparing dataset celeb_a/2.0.1 (download: 1.38 GiB, generated: 1.62 GiB, total: 3.00 GiB) to /root/tensorflow_datasets/celeb_a/2.0.1...\u001b[0m\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93bf01f74a124edd81488bff6561ad46","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"059815df872d490dad8bc61e6131a09c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"error","ename":"NonMatchingChecksumError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNonMatchingChecksumError\u001b[0m                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-a8d53b4cef91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mshuffle_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    385\u001b[0m           self._download_and_prepare(\n\u001b[1;32m    386\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m               download_config=download_config)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m           \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m   1023\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m         prepare_split_kwargs)\n\u001b[1;32m    961\u001b[0m     for split_generator in self._split_generators(\n\u001b[0;32m--> 962\u001b[0;31m         dl_manager, **split_generators_kwargs):\n\u001b[0m\u001b[1;32m    963\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/image/celeba.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;34m\"list_eval_partition\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEVAL_LIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;34m\"list_attr_celeba\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mATTR_DATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;34m\"landmarks_celeba\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLANDMARKS_DATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     })\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;31m# Add progress bar to follow the download state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_map_promise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0miter_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait promises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait promises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36m_target_settled_value\u001b[0;34m(self, _raise)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# type: (bool) -> Any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36m_settled_value\u001b[0;34m(self, _raise)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mraise_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/promise/promise.py\u001b[0m in \u001b[0;36mtry_catch\u001b[0;34m(handler, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# type: (Callable, Any, Any) -> Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36mcallback\u001b[0;34m(url_info)\u001b[0m\n\u001b[1;32m    480\u001b[0m           \u001b[0mtmp_dir_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_dir_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m           \u001b[0murl_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m           \u001b[0murl_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m       )\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m_handle_download_result\u001b[0;34m(self, resource, tmp_dir_path, url_path, url_info)\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;31m# Otherwise, missing checksums, do nothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0murl_info\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url_infos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mNonMatchingChecksumError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNonMatchingChecksumError\u001b[0m: Artifact https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM, downloaded to /root/tensorflow_datasets/downloads/ucexport_download_id_0B7EVK8r0v71pZjFTYXZWM3FlDDaXUAQO8EGH_a7VqGNLRtW52mva1LzDrb-V723OQN8.tmp.e79958dc222b4a408e294a6f6e9b91ab/uc, has wrong checksum. This might indicate:\n * The website may be down (e.g. returned a 503 status code). Please check the url.\n * For Google Drive URLs, try again later as Drive sometimes rejects downloads when too many people access the same URL. See https://github.com/tensorflow/datasets/issues/1482\n * The original datasets files may have been updated. In this case the TFDS dataset builder should be updated to use the new files and checksums. Sorry about that. Please open an issue or send us a PR with a fix.\n * If you're adding a new dataset, don't forget to register the checksums as explained in: https://www.tensorflow.org/datasets/add_dataset#2_run_download_and_prepare_locally\n"]}]},{"cell_type":"markdown","metadata":{"id":"13IpHMCxtVeA"},"source":["Second try:\n","\n","Load the data from GoogleDrive => succeed\n","\n","Problem: doesn't have attributes, cannot load as tensorflow image data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5p_xyjhfKDf","executionInfo":{"status":"ok","timestamp":1619944295227,"user_tz":-120,"elapsed":58191,"user":{"displayName":"Xiying Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5qAohoDgkfB_l_JjwVU3bU1D3p1gYIAZX2aBbXA=s64","userId":"17944844107397903410"}},"outputId":"82e41dff-f267-4c27-e103-984201879dbf"},"source":["# Connect colab with GoogleDrive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCazj33mzRV0"},"source":["local_zip = '/content/gdrive/MyDrive/HSLU/DLV/img_align_celeba.zip'\n","\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","\n","zip_ref.extractall('/tmp/celeba')\n","zip_ref.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HOSpZFhJfMgm"},"source":["# Load data from zip file saved in the GoogleDrive (takes less than 2mins)\n","# not using it anymore\n","# !unzip gdrive/My\\ Drive/HSLU/DLV/img_align_celeba"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a93-ayhKuNPn"},"source":["## Split the data into train and test datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TzUU9r-NupsD","executionInfo":{"status":"ok","timestamp":1619944426513,"user_tz":-120,"elapsed":837,"user":{"displayName":"Xiying Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5qAohoDgkfB_l_JjwVU3bU1D3p1gYIAZX2aBbXA=s64","userId":"17944844107397903410"}},"outputId":"d2a40ca7-bdbf-4b4e-fc10-9700746d9075"},"source":["# check that images are loaded\n","print(len(os.listdir('/tmp/celeba/img_align_celeba/')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["202599\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8wpXkl41upeC"},"source":["# create file folders to store the data\n","try:\n","  os.mkdir('/tmp/celeba/image')\n","  os.mkdir('/tmp/celeba/image/train')\n","  os.mkdir('/tmp/celeba/image/test')\n","\n","except OSError:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ga94hJqgNpui","executionInfo":{"status":"ok","timestamp":1619944503908,"user_tz":-120,"elapsed":1471,"user":{"displayName":"Xiying Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5qAohoDgkfB_l_JjwVU3bU1D3p1gYIAZX2aBbXA=s64","userId":"17944844107397903410"}},"outputId":"4b0478ee-f57c-423e-c6c0-1e1d33d6f245"},"source":["# check if folders are correctly created\n","print(len(os.listdir('/tmp/celeba/image/train')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fL68mjkNxcTE"},"source":["# split the dataset into train and test sets with 80/20 ratio\n","def split_data(SOURCE, TRAIN, TEST, SPLIT_SIZE):\n","  os.chdir(SOURCE)\n","  image_list = os.listdir(SOURCE)\n","  random.sample(image_list, len(image_list))\n","  train_size = len(image_list)*SPLIT_SIZE\n","\n","  for i in range(len(image_list)):\n","    if os.path.getsize(image_list[i]) != 0:\n","      if i < train_size:\n","        shutil.copy(image_list[i],TRAIN)\n","      if i >= train_size:\n","        shutil.copy(image_list[i], TEST)\n","    else:\n","      print(image[i] + ' is zero length, so ignoring')\n","\n","IMAGE_SOURCE_DIR = '/tmp/celeba/img_align_celeba'\n","TRAIN_DIR = '/tmp/celeba/image/train'\n","TEST_DIR = '/tmp/celeba/image/test'\n","split_size = 0.8\n","\n","split_data(IMAGE_SOURCE_DIR, TRAIN_DIR, TEST_DIR, split_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pUgN4Fy34-9b","executionInfo":{"status":"ok","timestamp":1619944622658,"user_tz":-120,"elapsed":880,"user":{"displayName":"Xiying Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5qAohoDgkfB_l_JjwVU3bU1D3p1gYIAZX2aBbXA=s64","userId":"17944844107397903410"}},"outputId":"3e79e199-c294-4575-9b2a-2ade16243865"},"source":["# check the result\n","print(len(os.listdir('/tmp/celeba/image/train/')))\n","print(len(os.listdir('/tmp/celeba/image/test/')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["162080\n","40519\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u425kfTw6j9s"},"source":["# Prepare the dataset"]},{"cell_type":"code","metadata":{"id":"Ec40R2vR4-6P"},"source":["# Define global constants to be used in this notebook\n","batch_size=128\n","img_height=178\n","img_width=218\n","latent_dim=2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o9nOSIXwPnTT","executionInfo":{"status":"ok","timestamp":1619948440196,"user_tz":-120,"elapsed":32282,"user":{"displayName":"Xiying Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg5qAohoDgkfB_l_JjwVU3bU1D3p1gYIAZX2aBbXA=s64","userId":"17944844107397903410"}},"outputId":"b2662efe-9330-4b7c-d632-40b2c4824cc2"},"source":["# the code here is wrong, it loads the data with two lables: 'train' and 'test'\n","# question: how to load the data correctly?\n","# note: also need to normalize the data by /255.\n","# question: need to reshape the images or not?\n","test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    '/tmp/celeba/image/',\n","    image_size=(img_height, img_width),\n","    batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 202599 files belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vKVET7zzsGSC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OpK7qQksGH_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7SvbI-Z5Wo8"},"source":["# Build VAE model\n","\n","## Sampling class"]},{"cell_type":"code","metadata":{"id":"KbK-80JZ4-wZ"},"source":["class Sampling(tf.keras.layers.Layer):\n","  def call(self, inputs):\n","    \"\"\"Generates a random sample and combines with the encoder output\n","    \n","    Args:\n","      inputs -- output tensor from the encoder\n","\n","    Returns:\n","      `inputs` tensors combined with a random sample\n","    \"\"\"\n","\n","    # unpack the output of the encoder\n","    mu, sigma = inputs\n","\n","    # get the size and dimensions of the batch\n","    batch = tf.shape(mu)[0]\n","    dim = tf.shape(mu)[1]\n","\n","    # generate a random tensor\n","    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n","\n","    # combine the inputs and noise\n","    return mu + tf.exp(0.5 * sigma) * epsilon"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qiypth3jfcaT"},"source":["## Encoder"]},{"cell_type":"code","metadata":{"id":"rXS-l7hE4-td"},"source":["def encoder_layers(inputs, latent_dim):\n","  \"\"\"Defines the encoder's layers.\n","  Args:\n","    inputs -- batch from the dataset\n","    latent_dim -- dimensionality of the latent space\n","\n","  Returns:\n","    mu -- learned mean\n","    sigma -- learned standard deviation\n","    batch_2.shape -- shape of the features before flattening\n","  \"\"\"\n","\n","  # add the Conv2D layers followed by BatchNormalization\n","  x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)\n","  x = tf.keras.layers.BatchNormalization()(x)\n","  x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)\n","\n","  # assign to a different variable so you can extract the shape later\n","  batch_2 = tf.keras.layers.BatchNormalization()(x)\n","\n","  # flatten the features and feed into the Dense network\n","  x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_2)\n","\n","  # arbitrarily used 20 units here but can change and see different results\n","  x = tf.keras.layers.Dense(20, activation='relu', name=\"encode_dense\")(x)\n","  x = tf.keras.layers.BatchNormalization()(x)\n","\n","  # add output Dense networks for mu and sigma, units equal to the declared latent_dim.\n","  mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n","  sigma = tf.keras.layers.Dense(latent_dim, name ='latent_sigma')(x)\n","\n","  return mu, sigma, batch_2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eEtZckl4-o6"},"source":["# define the encoder model that includes the Sampling layer\n","def encoder_model(latent_dim, input_shape):\n","  \"\"\"Defines the encoder model with the Sampling layer\n","  Args:\n","    latent_dim -- dimensionality of the latent space\n","    input_shape -- shape of the dataset batch\n","\n","  Returns:\n","    model -- the encoder model\n","    conv_shape -- shape of the features before flattening\n","  \"\"\"\n","\n","  # declare the inputs tensor with the given shape\n","  inputs = tf.keras.layers.Input(shape=input_shape)\n","\n","  # get the output of the encoder_layers() function\n","  mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=latent_dim)\n","\n","  # feed mu and sigma to the Sampling layer\n","  z = Sampling()((mu, sigma))\n","\n","  # build the whole encoder model\n","  model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n","\n","  return model, conv_shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MBTXKmfzf3kY"},"source":["## Decoder"]},{"cell_type":"code","metadata":{"id":"8W7veXau4-lU"},"source":["# Decoder expands the latent representations back to the original image dimensions\n","def decoder_layers(inputs, conv_shape):\n","  \"\"\"Defines the decoder layers.\n","  Args:\n","    inputs -- output of the encoder \n","    conv_shape -- shape of the features before flattening\n","\n","  Returns:\n","    tensor containing the decoded output\n","  \"\"\"\n","\n","  # feed to a Dense network with units computed from the conv_shape dimensions\n","  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n","  x = tf.keras.layers.Dense(units, activation = 'relu', name=\"decode_dense1\")(inputs)\n","  x = tf.keras.layers.BatchNormalization()(x)\n","  \n","  # reshape output using the conv_shape dimensions\n","  x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n","\n","  # upsample the features back to the original dimensions\n","  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_2\")(x)\n","  x = tf.keras.layers.BatchNormalization()(x)\n","  x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_3\")(x)\n","  x = tf.keras.layers.BatchNormalization()(x)\n","  x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid', name=\"decode_final\")(x)\n","  \n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WEWEEIT4-hl"},"source":["# define the Decoder model\n","def decoder_model(latent_dim, conv_shape):\n","  \"\"\"Defines the decoder model.\n","  Args:\n","    latent_dim -- dimensionality of the latent space\n","    conv_shape -- shape of the features before flattening\n","\n","  Returns:\n","    model -- the decoder model\n","  \"\"\"\n","\n","  # set the inputs to the shape of the latent space\n","  inputs = tf.keras.layers.Input(shape=(latent_dim,))\n","\n","  # get the output of the decoder layers\n","  outputs = decoder_layers(inputs, conv_shape)\n","\n","  # declare the inputs and outputs of the model\n","  model = tf.keras.Model(inputs, outputs)\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDUeOnW8ln8D"},"source":["## Kullback-Leibler Divergence for loss function"]},{"cell_type":"code","metadata":{"id":"9tc19ksP4-Z3"},"source":["# Kullback_leibler Divergence is added to the reconstruction loss,\n","# to consider the random normal distribution introduced in the latent layer\n","def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n","  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n","  Args:\n","    inputs -- batch from the dataset\n","    outputs -- output of the Sampling layer\n","    mu -- mean\n","    sigma -- standard deviation\n","\n","  Returns:\n","    KLD loss\n","  \"\"\"\n","  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n","  kl_loss = tf.reduce_mean(kl_loss) * -0.5\n","\n","  return kl_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"55ntgJHxl-Rw"},"source":["## VAE Model"]},{"cell_type":"code","metadata":{"id":"5l68fMmm4-RQ"},"source":["def vae_model(encoder, decoder, input_shape):\n","  \"\"\"Defines the VAE model\n","  Args:\n","    encoder -- the encoder model\n","    decoder -- the decoder model\n","    input_shape -- shape of the dataset batch\n","\n","  Returns:\n","    the complete VAE model\n","  \"\"\"\n","\n","  # set the inputs\n","  inputs = tf.keras.layers.Input(shape=input_shape)\n","\n","  # get mu, sigma, and z from the encoder output\n","  mu, sigma, z = encoder(inputs)\n","  \n","  # get reconstructed output from the decoder\n","  reconstructed = decoder(z)\n","\n","  # define the inputs and outputs of the VAE\n","  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n","\n","  # add the KL loss\n","  loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n","  model.add_loss(loss)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GjHFzDAd4-Nz"},"source":["# add a function to setup and get the different models\n","def get_models(input_shape, latent_dim):\n","  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n","  encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n","  decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n","  vae = vae_model(encoder, decoder, input_shape=input_shape)\n","  return encoder, decoder, vae"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOEtY80B4-Kp"},"source":["# Get the encoder, decoder and 'master' model (called vae)\n","encoder, decoder, vae = get_models(input_shape=(178,218,3,), latent_dim=latent_dim)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xf8IyMc2mcJu"},"source":["# Train the model"]},{"cell_type":"code","metadata":{"id":"m9u-nQ6amX5Z"},"source":["# define loss function, optimizer, and metrics\n","optimizer = tf.keras.optimizers.Adam()\n","loss_metric = tf.keras.metrics.Mean()\n","bce_loss = tf.keras.losses.BinaryCrossentropy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3McuNE0-mX2K"},"source":["# add a function to show the progress of the image generation\n","def generate_and_save_images(model, epoch, step, test_input):\n","  \"\"\"Helper function to plot our 16 images\n","\n","  Args:\n","\n","  model -- the decoder model\n","  epoch -- current epoch number during training\n","  step -- current step number during training\n","  test_input -- random tensor with shape (16, latent_dim)\n","  \"\"\"\n","\n","  # generate images from the test input\n","  predictions = model.predict(test_input)\n","\n","  # plot the results\n","  fig = plt.figure(figsize=(4,4))\n","\n","  for i in range(predictions.shape[0]):\n","      plt.subplot(4, 4, i+1)\n","      plt.imshow(predictions[i, :, :, 0])\n","      plt.axis('off')\n","\n","  # tight_layout minimizes the overlap between 2 sub-plots\n","  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n","  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rCZpqtqcm6UW"},"source":["Below is the training loop."]},{"cell_type":"code","metadata":{"id":"M-DVOQcMmXyI"},"source":["\n","# generate random vector as test input to the decoder\n","random_vector_for_generation = tf.random.normal(shape=[16, latent_dim])\n","\n","# number of epochs\n","epochs = 50\n","\n","# initialize the helper function to display outputs from an untrained model\n","generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n","\n","for epoch in range(epochs):\n","  print('Start of epoch %d' % (epoch,))\n","\n","  # iterate over the batches of the dataset.\n","  for step, x_batch_train in enumerate(train_dataset):\n","    with tf.GradientTape() as tape:\n","\n","      # feed a batch to the VAE model\n","      reconstructed = vae(x_batch_train)\n","\n","      # compute reconstruction loss\n","      # 38804=218*178\n","      flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n","      flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n","      loss = bce_loss(flattened_inputs, flattened_outputs) * 38804 \n","      \n","      # add KLD regularization loss\n","      loss += sum(vae.losses)  \n","\n","    # get the gradients and update the weights\n","    grads = tape.gradient(loss, vae.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n","\n","    # compute the loss metric\n","    loss_metric(loss)\n","\n","    # display outputs every 100 steps\n","    if step % 2 == 0:\n","      display.clear_output(wait=False)    \n","      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n","      print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"],"execution_count":null,"outputs":[]}]}